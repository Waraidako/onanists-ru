<!DOCTYPE html>
<head>
<meta charset="utf-8"></meta>
    <style>
        @font-face {
            font-family: math-font;
            src: url(https://fred-wang.github.io/MathFonts/TeXGyreSchola/texgyreschola-math.woff2);
        }
        math {
            font-family: math-font;
        }
        body{
            font-family:Calibri;
            max-width: 50em;
            margin: auto;
            padding: 10px;
            background-color: white;
        }
        table{
            margin: 1em 0 1em 0;
        }
        table,tr,th,td{
            border: 1px solid #474747;
            border-collapse: collapse;
            text-align: left;
            vertical-align: top;
        }
        img{
            max-width: 100%;
        }
        a {
            color: #287cff
        }
    </style>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
</head><h2 id="следующая-пара---лабы-скипнешь---въебу">Следующая пара - лабы,
скипнешь - въебу</h2>
<p>Идеи А.Н. Колмогорова:</p>
<ul>
<li>Энтропия - мера сложности объекта</li>
<li>Сложность объекта по Колмогорову - длина алгорита, реализованного
машиной Тьюринга, описывающей объект</li>
</ul>
<h4 id="энтропия-языка">Энтропия языка</h4>
<p>Нулвеое приближение - берём 32 буквы русского алфавита и пробел,
поместим их в "ящик" и будем составлять их них текста так - перемешаем
быквы, достанем одну, запишем, положим обратно и перемашаем ещё раз.
Итог - что-то вроде этого:<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326091126.png"
alt="Pasted image 20250326091126.png" /><br />
Первое приближение - будем учитывать частоты каждой из букв<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326091159.png"
alt="Pasted image 20250326091159.png" /><br />
Второе приближение - учитываем частоты диаграмм<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326091219.png"
alt="Pasted image 20250326091219.png" /><br />
Третье приближение - учитываем частоты триграмм<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326091239.png"
alt="Pasted image 20250326091239.png" /><br />
Четвёртое приближение - учитываем частоты тетраграмм<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326091300.png"
alt="Pasted image 20250326091300.png" /></p>
<h4
id="роль-вероятностных-параметров-слов-для-измерения-содержащейся-в-тексте-информации">Роль
вероятностных параметров слов для измерения содержащейся в тексте
информации</h4>
<p>Первое приближение - учтены частоты появления слов<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326085256.png"
alt="Pasted image 20250326085256.png" /><br />
Второе приближение - учитываются частоты сочетаний двух соседних
слов<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326085307.png"
alt="Pasted image 20250326085307.png" /></p>
<h4 id="свойства-энтропии-языка">Свойства энтропии языка</h4>
<p>Энтропией можно обозначить меру сложности объекта, в том числе
языка<br />
<strong>Первое свойство энтропии языка</strong> - Если энтропия языка
равна
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>,
то существует примерно
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>2</mn><mrow><mi>H</mi><mi>k</mi></mrow></msup><annotation encoding="application/x-tex">2^{Hk}</annotation></semantics></math>
текстов длиной
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>,
принадлежащих данному языку<br />
<strong>Второе свойство энтропии языка</strong> - если энтропия языка
равна
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>,
то при оптимальном способе кодирования каждый текст языка удлинится в
среднем в
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>
раз<br />
<strong>Третье свойство</strong> - (Пусть каждому тексту языка
соответствует вероятность - вероятность события, что из всех мыслимых
текстов заданной длины появится именно этот) Если энтропия языка равна
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>,
то для подавляющего большниства текстов длины
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
такая вероятность равна
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>2</mn><mrow><mi>−</mi><mi>H</mi><mi>k</mi></mrow></msup><annotation encoding="application/x-tex">2^{-Hk}</annotation></semantics></math><br />
<strong>Энтропия русского языка</strong> - по Колмогорову 1.33<br />
Энтропия английскиго языка по Шеннону - 0.6-1.33</p>
<p><img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326090029.png"
alt="Pasted image 20250326090029.png" /><br />
Чем меньше корпус допустимых текстов, тем меньше величина энтропии
языка</p>
<p><strong>Тексты языка</strong>:</p>
<ul>
<li>Актуальные - реально существующие к данному моменту времени тексты
на данным языке</li>
<li>Потенциальные - все возможные тексты</li>
</ul>
<p>Мы рассматриваем все тексты как потенциальные</p>
<h4 id="остаточная-энтропия">Остаточная энтропия</h4>
<p>Пусть каждое предложение иностранного языка можно первести на русский
n способами (среднее количество переводов). Текст из 100 предложений
можно перевести
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>n</mi><mn>100</mn></msup><annotation encoding="application/x-tex">n^{100}</annotation></semantics></math>
способами<br />
Остаточная энтропия по Колмогорову<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326090416.png"
alt="Pasted image 20250326090416.png" /></p>
<p>Рассмотрим два языка - полный русский (энтропия А) и ограниченный
русский с энтропией B,<br />
Тогда примерные количестве текстов длины k N1 &amp; N2 будут равны<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326090531.png"
alt="Pasted image 20250326090531.png" /><br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326090537.png"
alt="Pasted image 20250326090537.png" /></p>
<p>Если для заданного текста имеется N переводов длины k, то допустимых
переводов этого текста должно быть в
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>2</mn><mrow><mi>α</mi><mi>k</mi></mrow></msup><annotation encoding="application/x-tex">2^{\alpha k}</annotation></semantics></math>
меньше<br />
Чтобы допустимые переводы существовали, должно выполнятся неравенство
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>≥</mo><msup><mn>2</mn><mrow><mi>α</mi><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">N\geqslant2^{\alpha k}</annotation></semantics></math>
или
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>≥</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">h\geqslant\alpha</annotation></semantics></math>,
где h - остаточная энтропия</p>
<h4 id="колмогоровская-сложность">Колмогоровская сложность</h4>
<p><strong>Условная сложность</strong> - сложность текста, вычисленная
при условии, что указанные в тексте данные уже известны и могут быть
использованы при составлении описаний<br />
Условная сложность &lt; Абсолютная сложность<br />
<strong>Удельная сложность</strong> - сложность целого текста,
поделённая на длину текста (сложность, в среднем приходящаяся на один
знак)<br />
Удельная сложность &lt; Энтропия языка | Для длинных текстов</p>
<h4 id="три-подхода-к-определению-понятия-количество-информации">Три
подхода к определению понятия "количество информации"</h4>
<ul>
<li>Комбинаторный подход по Колмогорову:<br />
Энтропия переменного x:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub><mi>N</mi></mrow><annotation encoding="application/x-tex">H(x)=log_2N</annotation></semantics></math><br />
Указывая на определённое значение, энтропия снимается сообщением
информации:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo>=</mo><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub><mi>N</mi></mrow><annotation encoding="application/x-tex">I=log_2N</annotation></semantics></math><br />
Интересен при кодировании информации<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326091923.png"
alt="Pasted image 20250326091923.png" /></li>
<li>Вероятностный подход по Колмогорову<br />
Колмогоров отмечает - придание переменным характера случайных
переменных, обладающих совместным распределением вероятностей, позволяет
получить значительно более богатую систему понятий и соотношений<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326092322.png"
alt="Pasted image 20250326092322.png" /><br />
(x и y - это сообщения)<br />
При вероятностном подходе можно образовать матожидания
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><msub><mi>H</mi><mi>w</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mi>/</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">MH_w(y/x)</annotation></semantics></math>
и
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><msub><mi>I</mi><mi>w</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>:</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">MI_w(x:x)</annotation></semantics></math><br />
Величина
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mi>w</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>M</mi><msub><mi>I</mi><mi>w</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>:</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>M</mi><msub><mi>I</mi><mi>w</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>:</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">I_w(x,y) = MI_w(x:y) = MI_w(y:x)</annotation></semantics></math>
характеризует “тесноту связи” между x и y симметричным образом
Колмогоров отмечает один парадокс:<br />
Величина
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>:</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">I(x:y)</annotation></semantics></math>
при комбинаторном подходе всегда неотрицательна (что естественно),
величина же
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mi>w</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>:</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">I_w(x:y)</annotation></semantics></math>
может быть и отрицательной<br />
Подлинной мерой “количества информации” теперь становится усредненная
величина
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mi>w</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">I_w(x,y)</annotation></semantics></math>
<ul>
<li>Алгоритмический подход по Колмогорову<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326092926.png"
alt="Pasted image 20250326092926.png" /><br />
Относительная сложность объекта y при заданном x - минимальная длина
I(p) программы p получения y из x<br />
Метод программирования
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">\phi(p,x)=y</annotation></semantics></math>,
где функцию
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\phi(p,x)</annotation></semantics></math>
считаем частично рекурсивной<br />
Для любой такой функции полагаем
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>ϕ</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mi>/</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>m</mi><mi>i</mi><mi>n</mi><mi>I</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">K_\phi(y/x)=min I(p)</annotation></semantics></math><br />
Если не существует p, удовлетворяющее
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">\phi(p,x)=y</annotation></semantics></math>,
то
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>ϕ</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mi>/</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">K_\phi(y/x)=\infty</annotation></semantics></math></li>
</ul></li>
</ul>
<h5 id="основная-теорема-колмогорова">Основная теорема Колмогорова</h5>
<p>Существует такая частично рекурсивная функци A(p,x), что для любой
другой частично рекурсивной функции
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\phi(p,x)</annotation></semantics></math>
выполняется:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>A</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><msub><mi>K</mi><mi>ϕ</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="false" form="prefix">|</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mi>C</mi><mi>ϕ</mi></msub></mrow><annotation encoding="application/x-tex">K_A(y|x)\leqslant K_\phi(y|x) + C_\phi</annotation></semantics></math>
, где
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mi>ϕ</mi></msub><annotation encoding="application/x-tex">C_\phi</annotation></semantics></math>
не зависит от x и y<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326093549.png"
alt="Pasted image 20250326093549.png" /><br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326093555.png"
alt="Pasted image 20250326093555.png" /><br />
Если конечное множество M из очень большого числа элементов N допускает
определение при помощи программы длины, пренебрежимо малой по сравнению
с log2N, то почти все элементы множества M имеют сложность K(x), близкую
к log2N. Элементы x ∈ M этой сложности и рассматриваются как случайные
элементы множества M</p>
<h4 id="теорема-о-симметрии-взаимной-информации">Теорема о симметрии
взаимной информации</h4>
<p>Соотношение H(x:y)=H(y:x) сохраняется при замене H на K и знака
равенства на знак "приблизительно равно"<br />
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>:</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>K</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>:</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>O</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>m</mi><mi>a</mi><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>ν</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">|</mo></mrow><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>ν</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">|</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mspace width="0.222em"></mspace><mi>г</mi><mi>д</mi><mi>е</mi><mspace width="0.222em"></mspace><msub><mi>ν</mi><mn>1</mn></msub><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msub><mi>ν</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">K(x : y) = K(y : x) + O ( log max(|ν_1| , . . . , |ν_n| )),\ где\ ν_1, . . . , ν_n</annotation></semantics></math>
суть компоненты кортежей x и y.</p>
<h4 id="теорема-об-относительном-описании">Теорема об относительном
описании</h4>
<p>Для любых двух слов a и b существует программа p, которая преобразует
a в b, при этом имеет минимальную возможную длину (то есть ее длина
равна K(b|a)) и при этом p имеет очень малую сложность относительно b.
Другими словами, можно вычислить некоторое «хэш-значение» b длины
K(b|a), которого достаточно для восстановления b при заданном слове
a</p>
<h4
id="практичексое-использование-колмогоровской-сложности">Практичексое
использование колмогоровской сложности</h4>
<p>Математически близость текстов S и T характеризуется неотрицательным
числом - расстоянием<br />
Расстояние d должно удовлетворять трем условиям:</p>
<ul>
<li>Расстояние неотрицательно d(S, T) ≥ 0, если d(S, T) = 0 → S = T</li>
<li>Расстояние не меняется от перестановки текстов d(S, T) = d(T,
S)</li>
<li>Для любых трех текстов S, N, T выполняется неравенство треугольника
d(S, T) ≤ d(S, N) + d(N, T)</li>
</ul>
<p>Сжатый файл - набор инструкций для разжимающей программы, который
позволяет без потерь восстановить исходный текст. Минимальное количество
информации, необходимое для восстановления текста - колмогоровская
сложность<br />
Колмогоровская сложность текста T - K(T)<br />
Чтобы определить сложность S относительно T, нужно “подклеить” S к концу
T и посмотреть, насколько хорошо эта добавка сжимается<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326094417.png"
alt="Pasted image 20250326094417.png" /></p>
<p>Относительная сложность не может служить метрикой, поскольку
нарушаются условия 2 и 3: Условие 2 часто нарушается, если взять текст S
маленькой длины и текст T большой:<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326094555.png"
alt="Pasted image 20250326094555.png" /><br />
Если для тех же текстов взять среднее арифметическое или геометрическое,
то получится нечто симметричное, но неудовлетворяющее условию 3.</p>
<p>Относительная сложность применима в классификации текстов по автору.
Метрика расстояния тогда имеет вид:<br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326094655.png"
alt="Pasted image 20250326094655.png" /></p>
<h4 id="метрика-схожести">Метрика схожести</h4>
<p>Колмогоровская сложность невычислима для конечных объектов, но её
можно заменить длиной сжатых через gzip/GenCompress объектов<br />
Эта идея получила развитие в статье "The Similarity Metric"<br />
Идеи из статьи выше нашли развитие в поисковой машине Google.
Пример:</p>
<ul>
<li>Общее количество проиндексированных страниц - 8'058'044'651</li>
<li>horse - 46'700'000 ссылок</li>
<li>rider - 12'200'000 ссылок</li>
<li>horse rider - 2'630'000 ссылок</li>
<li>Нормированное семантическое расстояние NGD(horse, rider) =
0.443</li>
</ul>
<p><img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326095105.png"
alt="Pasted image 20250326095105.png" /><br />
<img
src="%D0%9F%D0%B8%D0%BA%D1%87%D0%B8/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D0%B8/Pasted%20image%2020250326095117.png"
alt="Pasted image 20250326095117.png" /><br />
NCD - будем искать на лабах</p>
